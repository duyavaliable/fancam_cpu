from ultralytics import YOLO
import cv2
import numpy as np
import os
import datetime
# from deepface import DeepFace
from numpy.linalg import norm
from moviepy.editor import VideoFileClip, AudioFileClip 
import time
import torch
import gc
import tensorflow as tf
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Táº¯t log TF thá»«a


# gpus = tf.config.experimental.list_physical_devices('GPU')
# if gpus:
#     try:
#         for gpu in gpus:
#             # Chá»‰ cho phÃ©p TF láº¥y dung lÆ°á»£ng cáº§n thiáº¿t, khÃ´ng chiáº¿m háº¿t 4GB
#             tf.config.experimental.set_memory_growth(gpu, True)
#     except RuntimeError as e:
#         print(e)


# Táº£i model (YOLOv8n) vÃ  cáº¥u hÃ¬nh device
# DEVICE = 0 if cv2.cuda.getCudaEnabledDeviceCount() > 0 else 'cpu' 
# model = YOLO("yolov8m.pt") 
# LOG_FILE = "fancam_error.log"
os.environ["CUDA_VISIBLE_DEVICES"] = "" 
DEVICE_STR = "cpu"
model = YOLO("yolov8n.pt")
LOG_FILE = "fancam_error.log"
CONFIDENCE_THRESHOLD = 0.3
EMA_ALPHA = 0.005
MAX_IOU_AREA_THRESHOLD = 2000
MAX_CENTER_DISTANCE = 500  # âœ… TÄ‚NG Tá»ª 300 â†’ 500 (cháº¥p nháº­n nháº£y xa hÆ¡n)
FACE_SIM_THRESHOLD = 0.65
MAX_LOST_FRAMES = 60  # âœ… THÃŠM Má»šI: NgÆ°á»¡ng fast-forward
FAST_FORWARD_INTERVAL = 5  # âœ… THÃŠM Má»šI: Check má»—i 5 frames


# --- HÃ€M TIá»†N ÃCH: GHI Lá»–I VÃ€O Tá»†P LOG ---
def log_error(step, error_message, level="ERROR"):
    """Ghi lá»—i hoáº·c cáº£nh bÃ¡o vÃ o tá»‡p log."""
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] [{level}] - STEP: {step} - MESSAGE: {error_message}\n"
    print(log_entry.strip()) # In ra console Ä‘á»ƒ debug nhanh
    try:
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(log_entry)
    except Exception as e:
        print(f"KhÃ´ng thá»ƒ ghi log vÃ o tá»‡p {LOG_FILE}: {e}")


# Bá»• sung cáº£nh bÃ¡o náº¿u dÃ¹ng CPU
# try:
#     # BUá»˜C THIáº¾T Bá»Š LÃ€ CUDA:0 (GPU 1 - Quadro P600)
#     # Náº¿u cÃ³ lá»—i, nÃ³ sáº½ bá»‹ báº¯t ngay láº­p tá»©c.
#     DEVICE_STR = "cuda:0" 
    
#     # Khá»Ÿi táº¡o model vÃ  táº£i nÃ³ vÃ o GPU
#     model = YOLO("yolov8n.pt") 
#     # Báº¯t buá»™c chuyá»ƒn model sang GPU ngay sau khi táº£i
#     model.to(DEVICE_STR) 
    
#     print(f"\nâœ… THÃ€NH CÃ”NG: Model Ä‘Ã£ Ä‘Æ°á»£c táº£i vÃ o {DEVICE_STR}\n")
    
# except Exception as e:
#     # Náº¿u tháº¥t báº¡i, chÃºng ta sáº½ buá»™c dÃ¹ng CPU vÃ  ghi lá»—i
#     DEVICE_STR = "cpu"
#     model = YOLO("yolov8n.pt") 
#     log_error("Model Setup", f"Lá»–I NGHIÃŠM TRá»ŒNG KHI KHá»I Táº O CUDA. Model Ä‘ang cháº¡y trÃªn CPU: {e}")
    
#     print("\n---------------------------------------------------------")
#     print("!!! KHá»I Táº O CUDA THáº¤T Báº I. CHÆ¯Æ NG TRÃŒNH CHáº Y TRÃŠN CPU !!!")
#     print("---------------------------------------------------------\n")

def is_blur(image, threshold=60):
    """Kiá»ƒm tra áº£nh cÃ³ bá»‹ nhÃ²e khÃ´ng Ä‘á»ƒ trÃ¡nh Re-ID nháº§m."""
    if image is None or image.size == 0: return True
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # TÃ­nh Ä‘á»™ biáº¿n thiÃªn Laplacian - cÃ¡ch nhanh nháº¥t Ä‘o Ä‘á»™ nÃ©t
    score = cv2.Laplacian(gray, cv2.CV_64F).var()
    return score < threshold

def enhance_face(face_img):
    """TÄƒng cháº¥t lÆ°á»£ng vÃ¹ng máº·t báº±ng thuáº­t toÃ¡n truyá»n thá»‘ng (khÃ´ng tá»‘n GPU)."""
    if face_img is None or face_img.size == 0: return face_img
    
    # 1. Resize vá» chuáº©n Facenet 160x160 Ä‘á»ƒ model xá»­ lÃ½ tá»‘t nháº¥t
    face_img = cv2.resize(face_img, (160, 160), interpolation=cv2.INTER_CUBIC)
    
    # 2. TÄƒng Ä‘á»™ sáº¯c nÃ©t báº±ng Unsharp Masking
    gaussian = cv2.GaussianBlur(face_img, (0, 0), 2.0)
    enhanced = cv2.addWeighted(face_img, 1.5, gaussian, -0.5, 0)
    
    # 3. CÃ¢n báº±ng Ã¡nh sÃ¡ng cá»¥c bá»™ (CLAHE) giÃºp máº·t rÃµ nÃ©t hÆ¡n
    lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    lab[:, :, 0] = clahe.apply(lab[:, :, 0])
    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)

def extract_embedding(image_data):
    """HÃ m trÃ­ch xuáº¥t embedding Ä‘Ã£ tá»‘i Æ°u hÃ³a cháº¥t lÆ°á»£ng áº£nh Ä‘áº§u vÃ o."""
    from deepface import DeepFace
    try:
        # Nháº­n dá»¯ liá»‡u áº£nh (cÃ³ thá»ƒ lÃ  Ä‘Æ°á»ng dáº«n string hoáº·c máº£ng numpy tá»« crop)
        img = cv2.imread(image_data) if isinstance(image_data, str) else image_data
        
        if img is None: return None
        
        # BÆ°á»›c 1: Bá» qua náº¿u áº£nh quÃ¡ má»
        if is_blur(img): return None
        
        # BÆ°á»›c 2: TÄƒng cÆ°á»ng cháº¥t lÆ°á»£ng áº£nh máº·t
        processed_face = enhance_face(img)
        
        # BÆ°á»›c 3: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng vá»›i DeepFace
        embedding = DeepFace.represent(
            img_path=processed_face,
            model_name="Facenet512",
            enforce_detection=False,
            align=True,           # Tá»± Ä‘á»™ng cÄƒn chá»‰nh máº¯t/mÅ©i Ä‘á»ƒ Re-ID chuáº©n hÆ¡n
            detector_backend='opencv' # DÃ¹ng backend nhanh nháº¥t cho GPU 4GB
        )[0]["embedding"]
        
        return np.array(embedding)
    except Exception as e:
        log_error("Face Embedding", f"Lá»—i trÃ­ch xuáº¥t embedding: {e}")
        return None

def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (norm(v1) * norm(v2) + 1e-8)

def handle_upload(new_files, current_files):
    """ThÃªm tá»‡p má»›i (new_files) vÃ o danh sÃ¡ch tá»‡p hiá»‡n táº¡i (current_files)."""
    if new_files is None:
        # TrÆ°á»ng há»£p ngÆ°á»i dÃ¹ng há»§y bá» há»™p thoáº¡i chá»n tá»‡p
        # Tráº£ vá» current_files cho State, vÃ  paths cho Output
        output_paths = [f.name for f in (current_files or [])]
        return current_files, output_paths
    
    if not isinstance(new_files, list):
        new_files = [new_files]
        
    # [QUAN TRá»ŒNG]: LÆ°u trá»¯ toÃ n bá»™ Ä‘á»‘i tÆ°á»£ng file trong State
    updated_files = (current_files or []) + new_files
    
    # [Sá»¬A Lá»–I]: Tráº£ vá» CHUá»–I ÄÆ¯á»œNG DáºªN cho Output hiá»ƒn thá»‹ (file_output)
    output_paths = [f.name for f in updated_files]
    
    # Tráº£ vá» danh sÃ¡ch Ä‘á»‘i tÆ°á»£ng file cho State, vÃ  danh sÃ¡ch path cho Output
    return updated_files, output_paths

# --- HÃ€M 1: PHÃT HIá»†N VÃ€ CHá»ŒN NGÆ¯á»œI BAN Äáº¦U (KhÃ´ng Ä‘á»•i) ---
def initial_detection(video_path):
    if not video_path:
        log_error("Initial Detection", "KhÃ´ng cÃ³ Ä‘Æ°á»ng dáº«n video Ä‘áº§u vÃ o.")
        return None, "Vui lÃ²ng táº£i lÃªn má»™t tá»‡p video.", []
    
    try:
        cap = cv2.VideoCapture(video_path)
        ret, frame = cap.read()
        cap.release()
    except Exception as e:
        log_error("Initial Detection", f"Lá»—i khi má»Ÿ/Ä‘á»c video: {e}")
        return None, "Lá»—i khi má»Ÿ/Ä‘á»c video.", []
    
    if not ret:
        log_error("Initial Detection", "KhÃ´ng thá»ƒ Ä‘á»c khung hÃ¬nh Ä‘áº§u tiÃªn (ret=False).")
        return None, "KhÃ´ng thá»ƒ Ä‘á»c khung hÃ¬nh tá»« video.", []

    try:
        # PhÃ¡t hiá»‡n ngÆ°á»i (person - class 0)
        results = model(frame, classes=0, device=DEVICE_STR, verbose=False)
        res = results[0]
        boxes = res.boxes.xyxy.cpu().numpy()
    except Exception as e:
        log_error("Initial Detection", f"Lá»—i trong quÃ¡ trÃ¬nh YOLOv8 phÃ¡t hiá»‡n: {e}")
        return None, "Lá»—i trong quÃ¡ trÃ¬nh phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng.", []

    # ... (CÃ¡c logic tÃ­nh toÃ¡n boxes, areas, detections váº«n giá»¯ nguyÃªn)
    if len(boxes) == 0:
        log_error("Initial Detection", "KhÃ´ng tÃ¬m tháº¥y ngÆ°á»i nÃ o trong khung hÃ¬nh Ä‘áº§u tiÃªn.")
        return None, "KhÃ´ng tÃ¬m tháº¥y ngÆ°á»i nÃ o trong khung hÃ¬nh Ä‘áº§u tiÃªn.", []

    areas = [(b[2]-b[0])*(b[3]-b[1]) for b in boxes]
    
    detections = []
    for i, box in enumerate(boxes):
        detections.append({
            'index': i + 1,
            'box': box,
            'area': areas[i]
        })

    detections.sort(key=lambda x: x['area'], reverse=True)
    default_target_box = detections[0]['box']
    
    sample_frame = frame.copy()
    info_list = []
    for det in detections:
        box = det['box'].astype(int)
        index = det['index']
        cv2.rectangle(sample_frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)
        cv2.putText(sample_frame, f'ID: {index}', (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
        info_list.append(f"ID {index} - Diá»‡n tÃ­ch: {int(det['area'])}")

    temp_img_path = "temp_detection.jpg"
    cv2.imwrite(temp_img_path, sample_frame)

    return temp_img_path, "ÄÃ£ phÃ¡t hiá»‡n ngÆ°á»i trong khung hÃ¬nh Ä‘áº§u tiÃªn. Vui lÃ²ng chá»n ID ngÆ°á»i báº¡n muá»‘n táº¡o fancam.", "\n".join(info_list)

def get_color_histogram(frame, box):
    """TrÃ­ch xuáº¥t mÃ u sáº¯c TOÃ€N THÃ‚N Ä‘á»ƒ tÄƒng Ä‘á»™ nháº­n diá»‡n khi Ä‘á»•i hÆ°á»›ng."""
    h_img, w_img = frame.shape[:2]
    x1, y1, x2, y2 = box.astype(int)
    
    # âœ… Láº¤Y TOÃ€N Bá»˜ BODY (30% - 90% chiá»u cao)
    y1_body = y1 + int((y2 - y1) * 0.3)
    y2_body = y1 + int((y2 - y1) * 0.9)
    
    crop = frame[max(0, y1_body):min(h_img, y2_body), max(0, x1):min(w_img, x2)]
    if crop.size == 0: return None
    
    hsv_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2HSV)
    
    # âœ… TÄ‚NG Sá» BIN (16x16 thay vÃ¬ 12x12)
    hist = cv2.calcHist([hsv_crop], [0, 1], None, [16, 16], [0, 180, 0, 256])
    cv2.normalize(hist, hist)
    return hist.flatten()

def compare_colors(hist1, hist2):
    """So sÃ¡nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng mÃ u sáº¯c (0 Ä‘áº¿n 1)."""
    if hist1 is None or hist2 is None: return 0
    # Sá»­ dá»¥ng HISTCMP_CORREL (TÆ°Æ¡ng quan) Ä‘á»ƒ cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao
    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)

def linear_interpolate_center(history, max_gap):
    """
    Ná»™i suy tuyáº¿n tÃ­nh cÃ¡c vá»‹ trÃ­ bá»‹ máº¥t dáº¥u (lost) trong tracking_history.
    """
    interpolated_centers = np.array([[h[1], h[2]] for h in history])
    i = 0
    while i < len(history):
        if not history[i][3]:  # TÃ¬m Ä‘iá»ƒm báº¯t Ä‘áº§u cá»§a khoáº£ng trá»‘ng (is_found=False)
            start_i = i - 1
            if start_i < 0:
                i += 1
                continue
                
            end_i = i
            while end_i < len(history) and not history[end_i][3]:
                end_i += 1
            
            if end_i == len(history):
                break
            
            gap_length = end_i - start_i
            
            # Chá»‰ ná»™i suy náº¿u khoáº£ng trá»‘ng Ä‘á»§ nhá» (táº¡o chuyá»ƒn Ä‘á»™ng camera mÆ°á»£t)
            if gap_length <= max_gap:
                center_A = interpolated_centers[start_i]
                center_B = interpolated_centers[end_i]
                
                for j in range(start_i + 1, end_i):
                    t = (j - start_i) / gap_length
                    interpolated_centers[j] = center_A + (center_B - center_A) * t
                i = end_i
            else:
                # Khoáº£ng trá»‘ng quÃ¡ lá»›n, giá»¯ nguyÃªn vá»‹ trÃ­ cuá»‘i cÃ¹ng Ä‘Æ°á»£c biáº¿t
                center_A = interpolated_centers[start_i]
                for j in range(start_i + 1, end_i):
                     interpolated_centers[j] = center_A
                i = end_i 
        i += 1
    return interpolated_centers





def get_color_histogram(frame, box):
    """TrÃ­ch xuáº¥t mÃ u sáº¯c TOÃ€N THÃ‚N Ä‘á»ƒ tÄƒng Ä‘á»™ nháº­n diá»‡n khi Ä‘á»•i hÆ°á»›ng."""
    h_img, w_img = frame.shape[:2]
    x1, y1, x2, y2 = box.astype(int)
    
    # âœ… Láº¤Y TOÃ€N Bá»˜ BODY (30% - 90% chiá»u cao)
    y1_body = y1 + int((y2 - y1) * 0.3)
    y2_body = y1 + int((y2 - y1) * 0.9)
    
    crop = frame[max(0, y1_body):min(h_img, y2_body), max(0, x1):min(w_img, x2)]
    if crop.size == 0: return None
    
    hsv_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2HSV)
    
    # âœ… TÄ‚NG Sá» BIN (16x16 thay vÃ¬ 12x12)
    hist = cv2.calcHist([hsv_crop], [0, 1], None, [16, 16], [0, 180, 0, 256])
    cv2.normalize(hist, hist)
    return hist.flatten()

def process_fancam(video_path, target_id_str, ref_face_paths, ref_color_paths, zoom_level):
    # --- 1. KHá»I Táº O & Äá»ŠNH NGHÄ¨A PATH (ÄÃ£ sá»­a lá»—i máº¥t path) ---
    kalman = cv2.KalmanFilter(4, 2)
    kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)
    kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)
    kalman.processNoiseCov = np.eye(4, dtype=np.float32) * 0.03
    
    output_path_temp = "temp_silent_fancam.avi"
    MAX_JUMP_DIST = 0.05  # Tá»· lá»‡ pháº§n trÄƒm tá»‘i Ä‘a cá»§a chiá»u rá»™ng video 
    log_error("Setup", "Báº¯t Ä‘áº§u tiáº¿n trÃ¬nh: Æ¯u tiÃªn Máº·t (20-60%) + MÃ u trung tÃ¢m (30-60%) + Vá»‹ trÃ­ (10-20%)", "INFO")

    # --- 2. TRÃCH XUáº¤T MáºªU ---
    ref_embedding = None
    if ref_face_paths:
        all_embs = [extract_embedding(f.name) for f in ref_face_paths if extract_embedding(f.name) is not None]
        if all_embs: ref_embedding = np.mean(all_embs, axis=0)

    ref_hist = None
    if ref_color_paths:
        sample_img = cv2.imread(ref_color_paths[0].name)
        res_c = model(sample_img, classes=0, conf=0.3, verbose=False)[0]
        if len(res_c.boxes) > 0:
            ref_hist = get_color_histogram(sample_img, res_c.boxes.xyxy.cpu().numpy()[0])

    # --- 3. VIDEO & KHá»I Táº O ID CHUáº¨N Táº I FRAME 1 ---
    cap_info = cv2.VideoCapture(video_path)
    width, height = int(cap_info.get(3)), int(cap_info.get(4))
    fps, total_frames = cap_info.get(5) or 30, int(cap_info.get(7))
    ret, frame_init = cap_info.read()
    cap_info.release()

    # QuÃ©t Re-ID ngay frame 1 Ä‘á»ƒ khá»›p ID tracker vá»›i ID má»¥c tiÃªu
    # res_i = model.track(frame_init, persist=True, classes=0, verbose=False)[0]
    res_i = model.track(frame_init, persist=True, classes=0, device=DEVICE_STR, imgsz=640, half=False, verbose=False)[0]
    boxes_i = res_i.boxes.xyxy.cpu().numpy()
    ids_i = res_i.boxes.id.cpu().numpy().astype(int) if res_i.boxes.id is not None else []
    
    current_target_id = -1
    for box, tid in zip(boxes_i, ids_i):
        f_crop = frame_init[int(box[1]):int(box[1]+(box[3]-box[1])*0.5), int(box[0]):int(box[2])]
        emb = extract_embedding(enhance_face(f_crop))
        if emb is not None and ref_embedding is not None:
            if cosine_similarity(ref_embedding, emb) > 0.7: 
                current_target_id = tid
                break

    if current_target_id == -1:
        try:
            # Chuyá»ƒn input cá»§a nÃ­ thÃ nh sá»‘ nguyÃªn
            target_idx = int(target_id_str) - 1 
            
            # Kiá»ƒm tra xem index cÃ³ náº±m trong danh sÃ¡ch khÃ´ng (trÃ¡nh lá»—i out of bounds)
            if 0 <= target_idx < len(ids_i):
                current_target_id = ids_i[target_idx]
                log_error("Init", f"DÃ¹ng ID ngÆ°á»i dÃ¹ng chá»n: {current_target_id}", "INFO")
            else:
                # Náº¿u nháº­p sá»‘ quÃ¡ lá»›n, máº·c Ä‘á»‹nh láº¥y ngÆ°á»i Ä‘áº§u tiÃªn (index 0)
                current_target_id = ids_i[0]
                log_error("Init", f"ID chá»n náº±m ngoÃ i danh sÃ¡ch ({len(ids_i)} ngÆ°á»i), láº¥y ID {current_target_id} máº·c Ä‘á»‹nh.", "WARN")
        except:
            return "Lá»—i: ID nháº­p vÃ o khÃ´ng há»£p lá»‡.", None

    idx_init = np.where(ids_i == current_target_id)[0][0]
    prev_cx, prev_cy = (boxes_i[idx_init][0]+boxes_i[idx_init][2])/2, (boxes_i[idx_init][1]+boxes_i[idx_init][3])/2
    kalman.statePost = np.array([[prev_cx], [prev_cy], [0], [0]], np.float32)

    # --- 4. PASS 1: SMART TRACKING ---
    tracking_history = []
    frame_count = 0
    lost_counter = 0  # âœ… BIáº¾N Äáº¾M Sá» FRAME Máº¤T Dáº¤U LIÃŠN Tá»¤C
    MAX_LOST_FRAMES = 60  # âœ… NGÆ¯á» NG CHUYá»‚N SANG FAST-FORWARD (60 frames = 2 giÃ¢y á»Ÿ 30fps)
    FAST_FORWARD_INTERVAL = 5  # âœ… KHI FAST-FORWARD, CHá»ˆ CHECK Má»–I 5 FRAMES
    
    results = model.track(source=video_path, tracker="bytetrack.yaml", persist=True, 
                          imgsz=384, classes=0, device=DEVICE_STR, stream=True, verbose=False)

    for res in results:
        frame_count += 1
        
        # âœ… HIá»‚N THá»Š TIáº¾N Äá»˜ Má»–I 50 FRAMES
        if frame_count % 50 == 0:
            log_error("Progress", f"â±ï¸ Frame {frame_count}/{total_frames} | Lost: {lost_counter}f", "INFO")
        
        frame = res.orig_img
        pred = kalman.predict()
        
        all_b = res.boxes.xyxy.cpu().numpy() if res.boxes.id is not None else []
        all_ids = res.boxes.id.cpu().numpy().astype(int) if res.boxes.id is not None else []

        found_box = None
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 1: KIá»‚M TRA ID HIá»†N Táº I (LUÃ”N CHáº Y Má»ŒI FRAME)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if current_target_id in all_ids:
            idx = np.where(all_ids == current_target_id)[0][0]
            curr_box = all_b[idx]
            curr_cx, curr_cy = (curr_box[0]+curr_box[2])/2, (curr_box[1]+curr_box[3])/2
            dist = np.sqrt((curr_cx - prev_cx)**2 + (curr_cy - prev_cy)**2)
            
            if dist < MAX_CENTER_DISTANCE:  # âœ… DÃ™NG 500px thay vÃ¬ 300px
                found_box = curr_box
                lost_counter = 0  # âœ… TÃŒM THáº¤Y â†’ RESET Äáº¾M
            else: 
                log_error("Motion", f"F{frame_count}: Nháº£y quÃ¡ xa ({int(dist)}px)", "WARN")
                lost_counter += 1  # âœ… NHáº¢Y XA QUÃ â†’ TÄ‚NG Äáº¾M

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 2: Xá»¬ LÃ KHI Máº¤T Dáº¤U (CHá»ˆ CHáº Y KHI found_box = None)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if found_box is None:
            lost_counter += 1
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # ğŸš€ MODE FAST-FORWARD (CHá»ˆ KÃCH HOáº T KHI lost_counter > 60)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            if lost_counter > MAX_LOST_FRAMES:
                # âœ… Bá» QUA 4/5 FRAMES (chá»‰ check frame 61, 66, 71, 76...)
                if (frame_count - MAX_LOST_FRAMES) % FAST_FORWARD_INTERVAL != 0:
                    tracking_history.append((frame_count, pred[0][0], pred[1][0], False))
                    continue  # â† NHáº¢Y QUA FRAME NÃ€Y, KHÃ”NG CHáº Y RE-ID
                
                log_error("FastForward", f"F{frame_count}: TÃ¬m láº¡i (máº¥t {lost_counter}f)...", "WARN")
                
                # âœ… QUÃ‰T TOÃ€N Bá»˜ KHUNG HÃŒNH - CHá»ˆ DÃ™NG MÃ€U Sáº®C
                best_score, best_id, temp_box = 0, None, None
                for box, tid in zip(all_b, all_ids):
                    c_s = compare_colors(ref_hist, get_color_histogram(frame, box)) if ref_hist is not None else 0
                    
                    # âœ… CHá»ˆ DÃ™NG COLOR (khÃ´ng Face, khÃ´ng Distance)
                    combined = c_s
                    
                    if combined > best_score: 
                        best_score, best_id, temp_box = combined, tid, box
                
                if best_id is not None and best_score > 0.35:  # âœ… NgÆ°á»¡ng tháº¥p
                    log_error("Success", f"F{frame_count}: âœ… TÃŒM Láº I ID {best_id} (Color: {best_score:.2f})", "INFO")
                    current_target_id = best_id
                    found_box = temp_box
                    lost_counter = 0  # âœ… TÃŒM Láº I ÄÆ¯á»¢C â†’ RESET
                else:
                    tracking_history.append((frame_count, pred[0][0], pred[1][0], False))
                    continue
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # ğŸ” MODE NORMAL (CHáº Y KHI lost_counter â‰¤ 60 HOáº¶C frame % 30 == 1)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            elif frame_count % 30 == 1:  # âœ… QUÃ‰T Ká»¸ Má»–I 1 GIÃ‚Y
                best_score, best_id, temp_box = 0, None, None
                for box, tid in zip(all_b, all_ids):
                    f_s = 0
                    x1, y1, x2, y2 = box.astype(int)
                    f_crop = frame[max(0,y1):min(height, y1+int((y2-y1)*0.5)), max(0,x1):min(width,x2)]
                    
                    blur_val = is_blur(f_crop, 90)
                    
                    # âœ… CHáº Y FACE RE-ID Náº¾U KHÃ”NG Má»œ + KHÃ”NG PHáº¢I CPU
                    if not blur_val and ref_embedding is not None and DEVICE_STR != "cpu":
                        emb = extract_embedding(enhance_face(f_crop))
                        if emb is not None: f_s = cosine_similarity(ref_embedding, emb)
                    
                    c_s = compare_colors(ref_hist, get_color_histogram(frame, box)) if ref_hist is not None else 0
                    
                    # âœ… Bá» TRá»ŒNG Sá» DISTANCE (vÃ¬ ngÆ°á»i cÃ³ thá»ƒ á»Ÿ xa)
                    if not blur_val and f_s > 0:
                        combined = (f_s * 0.6) + (c_s * 0.4)  # Face 60%, Color 40%
                    else:
                        combined = c_s  # Chá»‰ dÃ¹ng mÃ u náº¿u má»
                    
                    if combined > best_score: 
                        best_score, best_id, temp_box = combined, tid, box

                if best_id is not None and best_score > 0.5:  # âœ… NgÆ°á»¡ng tháº¥p hÆ¡n (0.5 thay vÃ¬ 0.65)
                    if current_target_id != best_id:
                        log_error("Success", f"F{frame_count}: Chá»‘t ID {best_id} (Score: {best_score:.2f})", "INFO")
                        current_target_id = best_id
                    found_box = temp_box
                    lost_counter = 0  # âœ… TÃŒM THáº¤Y â†’ RESET

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 3: LÆ¯U Káº¾T QUáº¢ VÃ€ Cáº¬P NHáº¬T KALMAN
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if found_box is not None:
            cx, cy = (found_box[0]+found_box[2])/2, (found_box[1]+found_box[3])/2
            kalman.correct(np.array([[np.float32(cx)], [np.float32(cy)]]))
            tracking_history.append((frame_count, cx, cy, True))
            prev_cx, prev_cy = cx, cy
        else:
            tracking_history.append((frame_count, pred[0][0], pred[1][0], False))
        
        # Giáº£i phÃ³ng bá»™ nhá»› Ä‘á»‹nh ká»³
        if frame_count % 100 == 0:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

    # --- 5. PASS 2: RENDER VIDEO (DYNAMIC EMA & SMART SMOOTHING) ---
    log_error("Render", f"Báº¯t Ä‘áº§u xá»­ lÃ½ Pass 2: Ná»™i suy & Smooth quá»¹ Ä‘áº¡o (Dynamic Alpha)...", "INFO")
    
    # 1. Ná»™i suy cÃ¡c vá»‹ trÃ­ bá»‹ máº¥t dáº¥u (Gaps)
    # max_gap=fps*0.5 nghÄ©a lÃ  náº¿u máº¥t dáº¥u dÆ°á»›i 0.5 giÃ¢y, mÃ¡y tá»± ná»‘i Ä‘iá»ƒm cÅ© vÃ  má»›i
    centers = linear_interpolate_center(tracking_history, max_gap=int(fps*0.5))
    
    # 2. Ãp dá»¥ng Dynamic EMA Ä‘á»ƒ camera khÃ´ng bá»‹ 'Ä‘uá»•i hÃ¬nh báº¯t bÃ³ng'
    smoothed_centers = [centers[0]]
    prev_smooth = centers[0]
    
    for i in range(1, len(centers)):
        curr_raw = centers[i]
        
        # TÃ­nh khoáº£ng cÃ¡ch dá»‹ch chuyá»ƒn (tá»‘c Ä‘á»™ tá»©c thá»i cá»§a Hanbin)
        dist = np.sqrt((curr_raw[0] - prev_smooth[0])**2 + (curr_raw[1] - prev_smooth[1])**2)
        
        # CÃ´ng thá»©c Dynamic Alpha:
        # Náº¿u Ä‘á»©ng yÃªn: alpha = 0.001 (siÃªu mÆ°á»£t)
        # Náº¿u di chuyá»ƒn: alpha tÄƒng dáº§n theo quÃ£ng Ä‘Æ°á»ng (max 0.1 Ä‘á»ƒ bÃ¡m ká»‹p zoom 2x)
        dynamic_alpha = np.clip(0.001 + (dist / width) * 0.8, 0.001, 0.1)
        
        # TÃ­nh vá»‹ trÃ­ má»›i dá»±a trÃªn trá»ng sá»‘ biáº¿n thiÃªn
        new_center = curr_raw * dynamic_alpha + prev_smooth * (1 - dynamic_alpha)
        smoothed_centers.append(new_center)
        prev_smooth = new_center

    centers = np.array(smoothed_centers)
    
    # Thiáº¿t láº­p thÃ´ng sá»‘ Crop cho Zoom 2x
    fancam_h, fancam_w = height, int(height * 9 / 16)
    crop_h, crop_w = height / zoom_level, (height / zoom_level) * (9 / 16)
    
    log_error("Render", f"Äang ghi tá»‡p video táº¡m thá»i: {output_path_temp}", "INFO")
    
    out_v = cv2.VideoWriter(output_path_temp, cv2.VideoWriter_fourcc(*"XVID"), fps, (fancam_w, fancam_h))
    cap_v = cv2.VideoCapture(video_path)
    
    for f_idx in range(total_frames):
        ret, f_orig = cap_v.read()
        if not ret: break
        
        # Láº¥y tá»a Ä‘á»™ tÃ¢m Ä‘Ã£ Ä‘Æ°á»£c lÃ m mÆ°á»£t
        c_idx = min(f_idx, len(centers) - 1)
        cx, cy = centers[c_idx]
        
        # TÃ­nh toÃ¡n tá»a Ä‘á»™ gÃ³c trÃ¡i trÃªn (Top-Left) Ä‘á»ƒ Crop
        l = int(np.clip(cx - crop_w/2, 0, width - int(crop_w)))
        t = int(np.clip(cy - crop_h/2, 0, height - int(crop_h)))
        
        # Thá»±c hiá»‡n Crop vÃ  Resize vá» 9:16 dá»c
        crop = f_orig[t:t+int(crop_h), l:l+int(crop_w)]
        if crop.size != 0:
            crop_res = cv2.resize(crop, (fancam_w, fancam_h), interpolation=cv2.INTER_LANCZOS4)
            out_v.write(crop_res)
        
        # Giáº£i phÃ³ng bá»™ nhá»› frame cÅ© ngay láº­p tá»©c
        del f_orig
        if f_idx % 100 == 0:
            gc.collect()

    cap_v.release()
    out_v.release()

    # --- 6. AUDIO SYNC & FINAL PACKAGING ---
    log_error("Audio", "Äang tiáº¿n hÃ nh Ä‘á»“ng bá»™ Ã¢m thanh gá»‘c vÃ  nÃ©n video chuáº©n H.264...", "INFO")
    
    f_out = os.path.basename(video_path).rsplit('.', 1)[0] + f"_fancam_final.mp4"
    
    try:
        # Load video cÃ¢m vÃ  audio gá»‘c
        v_clip = VideoFileClip(output_path_temp)
        a_clip = AudioFileClip(video_path)
        
        # GÃ¡n audio vÃ  Ã©p thá»i lÆ°á»£ng (duration) báº±ng nhau Ä‘á»ƒ trÃ¡nh lá»‡ch tiáº¿ng
        final_clip = v_clip.set_duration(a_clip.duration).set_audio(a_clip)
        
        # Xuáº¥t file cuá»‘i cÃ¹ng
        final_clip.write_videofile(
            f_out, 
            codec='libx264', 
            audio_codec='aac', 
            fps=fps, 
            preset='medium', # CÃ¢n báº±ng giá»¯a tá»‘c Ä‘á»™ vÃ  dung lÆ°á»£ng cho mÃ¡y yáº¿u
            logger=None
        )
        
        # Dá»n dáº¹p tá»‡p táº¡m
        v_clip.close(); a_clip.close()
        if os.path.exists(output_path_temp):
            os.remove(output_path_temp)
            
        log_error("Success", f"Fancam Ä‘Ã£ hoÃ n thÃ nh rá»±c rá»¡! ÄÆ°á»ng dáº«n: {f_out}", "INFO")
        return "Táº¡o Fancam thÃ nh cÃ´ng!", os.path.abspath(f_out)
        
    except Exception as e:
        log_error("Audio", f"Lá»—i trong bÆ°á»›c Ä‘Ã³ng gÃ³i cuá»‘i cÃ¹ng: {str(e)}", "ERROR")
        if os.path.exists(output_path_temp):
            return f"Video Ä‘Ã£ táº¡o (khÃ´ng tiáº¿ng) táº¡i: {output_path_temp}", os.path.abspath(output_path_temp)
        return "Lá»—i Ä‘á»“ng bá»™ Ã¢m thanh.", None
    